# Base Docker Compose Configuration for Alphintra Trading Platform
# This file contains the core infrastructure services that are common across all environments

# Remove this line: version: '3.8'

x-common-variables: &common-variables
  POSTGRES_MULTIPLE_DATABASES: "auth_db,trading_db,strategy_db,risk_db,analytics_db"
  KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
  KAFKA_NUM_PARTITIONS: 3
  KAFKA_DEFAULT_REPLICATION_FACTOR: 1

x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "3"

services:
  # ===============================
  # CORE INFRASTRUCTURE SERVICES
  # ===============================

  # PostgreSQL - Simulates GCP Cloud SQL
  postgres:
    image: postgres:15-alpine
    container_name: ${COMPOSE_PROJECT_NAME:-alphintra}-postgres
    environment:
      <<: *common-variables
      POSTGRES_DB: ${POSTGRES_DB:-alphintra}
      POSTGRES_USER: ${POSTGRES_USER:-alphintra}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-alphintra123}
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./databases/postgresql:/docker-entrypoint-initdb.d
      - ./monitoring/postgres-exporter:/etc/postgres_exporter
    networks:
      - alphintra-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-alphintra}"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging: *default-logging
    restart: unless-stopped

  # TimescaleDB - For time-series market data (GCP equivalent: Cloud SQL + extensions)
  timescaledb:
    image: timescale/timescaledb:latest-pg15
    container_name: ${COMPOSE_PROJECT_NAME:-alphintra}-timescaledb
    environment:
      POSTGRES_DB: ${TIMESCALE_DB:-timescaledb}
      POSTGRES_USER: ${TIMESCALE_USER:-timescale}
      POSTGRES_PASSWORD: ${TIMESCALE_PASSWORD:-timescale123}
    ports:
      - "${TIMESCALE_PORT:-5433}:5432"
    volumes:
      - timescaledb_data:/var/lib/postgresql/data
      - ./databases/timescaledb:/docker-entrypoint-initdb.d
    command: ["-c", "shared_preload_libraries=timescaledb"]
    networks:
      - alphintra-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${TIMESCALE_USER:-timescale}"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging: *default-logging
    restart: unless-stopped

  # Redis Cluster - Simulates GCP Cloud Memorystore
  redis-master:
    image: redis:7-alpine
    container_name: ${COMPOSE_PROJECT_NAME:-alphintra}-redis-master
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD:-redis123} --maxmemory 256mb --maxmemory-policy allkeys-lru
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - redis_master_data:/data
      - ./config/redis/redis.conf:/usr/local/etc/redis/redis.conf
    networks:
      - alphintra-network
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging: *default-logging
    restart: unless-stopped

  redis-replica:
    image: redis:7-alpine
    container_name: ${COMPOSE_PROJECT_NAME:-alphintra}-redis-replica
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD:-redis123} --replicaof redis-master 6379 --masterauth ${REDIS_PASSWORD:-redis123}
    ports:
      - "${REDIS_REPLICA_PORT:-6380}:6379"
    volumes:
      - redis_replica_data:/data
    networks:
      - alphintra-network
    depends_on:
      - redis-master
    healthcheck:
      test: ["CMD", "redis-cli", "-p", "6379", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging: *default-logging
    restart: unless-stopped

  # ===============================
  # EVENT STREAMING & MESSAGING
  # ===============================

  # Zookeeper - For Kafka coordination
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: ${COMPOSE_PROJECT_NAME:-alphintra}-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
    ports:
      - "${ZOOKEEPER_PORT:-2181}:2181"
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_logs:/var/lib/zookeeper/log
    networks:
      - alphintra-network
    logging: *default-logging
    restart: unless-stopped

  # Kafka - Simulates GCP Pub/Sub + Kafka
  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: ${COMPOSE_PROJECT_NAME:-alphintra}-kafka
    depends_on:
      - zookeeper
    ports:
      - "${KAFKA_PORT:-9092}:9092"
      - "${KAFKA_EXTERNAL_PORT:-9094}:9094"
    environment:
      <<: *common-variables
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:${KAFKA_PORT:-9092}
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:${KAFKA_PORT:-9092}
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_JMX_PORT: 9999
      KAFKA_JMX_HOSTNAME: localhost
    volumes:
      - kafka_data:/var/lib/kafka/data
      - ./config/kafka:/etc/kafka/configs
    networks:
      - alphintra-network
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging: *default-logging
    restart: unless-stopped

  # GCP Pub/Sub Emulator
  pubsub-emulator:
    image: google/cloud-sdk:alpine
    container_name: ${COMPOSE_PROJECT_NAME:-alphintra}-pubsub-emulator
    command: gcloud beta emulators pubsub start --host-port=0.0.0.0:8085
    ports:
      - "${PUBSUB_EMULATOR_PORT:-8085}:8085"
    environment:
      PUBSUB_PROJECT_ID: ${GCP_PROJECT_ID:-alphintra-local}
    networks:
      - alphintra-network
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "8085"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging: *default-logging
    restart: unless-stopped

  # ===============================
  # ML/AI INFRASTRUCTURE
  # ===============================

  # MLflow - Simulates GCP Vertex AI Model Registry
  mlflow:
    image: python:3.11-slim
    container_name: ${COMPOSE_PROJECT_NAME:-alphintra}-mlflow
    working_dir: /mlflow
    command: >
      bash -c "
        pip install mlflow[extras] psycopg2-binary boto3 google-cloud-storage &&
        mlflow server 
          --backend-store-uri postgresql://${POSTGRES_USER:-alphintra}:${POSTGRES_PASSWORD:-alphintra123}@postgres:5432/${POSTGRES_DB:-alphintra}
          --default-artifact-root gs://mlflow-artifacts-bucket
          --host 0.0.0.0 
          --port 5001
          --serve-artifacts
      "
    ports:
      - "${MLFLOW_PORT:-5001}:5001"
    volumes:
      - mlflow_data:/mlflow/mlruns
      - mlflow_artifacts:/mlflow/artifacts
      - ./config/mlflow:/mlflow/config
    networks:
      - alphintra-network
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      MLFLOW_BACKEND_STORE_URI: postgresql://${POSTGRES_USER:-alphintra}:${POSTGRES_PASSWORD:-alphintra123}@postgres:5432/${POSTGRES_DB:-alphintra}
      MLFLOW_DEFAULT_ARTIFACT_ROOT: ./artifacts
      GOOGLE_CLOUD_PROJECT: ${GCP_PROJECT_ID:-alphintra-local}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging: *default-logging
    restart: unless-stopped

  # MinIO - Simulates Google Cloud Storage
  minio:
    image: minio/minio:latest
    container_name: ${COMPOSE_PROJECT_NAME:-alphintra}-minio
    command: server /data --console-address ":9001"
    ports:
      - "${MINIO_PORT:-9000}:9000"
      - "${MINIO_CONSOLE_PORT:-9001}:9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ACCESS_KEY:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_SECRET_KEY:-minioadmin}
    volumes:
      - minio_data:/data
    networks:
      - alphintra-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging: *default-logging
    restart: unless-stopped

  # ===============================
  # MONITORING & OBSERVABILITY
  # ===============================

  # Prometheus - Metrics collection
  prometheus:
    image: prom/prometheus:latest
    container_name: ${COMPOSE_PROJECT_NAME:-alphintra}-prometheus
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./monitoring/prometheus/rules:/etc/prometheus/rules
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    networks:
      - alphintra-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging: *default-logging
    restart: unless-stopped

  # Grafana - Dashboards and visualization
  grafana:
    image: grafana/grafana:latest
    container_name: ${COMPOSE_PROJECT_NAME:-alphintra}-grafana
    ports:
      - "${GRAFANA_PORT:-3001}:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:-admin123}
      GF_USERS_ALLOW_SIGN_UP: false
      GF_INSTALL_PLUGINS: grafana-clock-panel,grafana-simple-json-datasource,grafana-worldmap-panel
      GF_FEATURE_TOGGLES_ENABLE: ngalert
    networks:
      - alphintra-network
    depends_on:
      - prometheus
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging: *default-logging
    restart: unless-stopped

  # Jaeger - Distributed tracing
  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: ${COMPOSE_PROJECT_NAME:-alphintra}-jaeger
    ports:
      - "${JAEGER_UI_PORT:-16686}:16686"
      - "${JAEGER_GRPC_PORT:-14250}:14250"
      - "${JAEGER_HTTP_PORT:-14268}:14268"
      - "${JAEGER_ZIPKIN_PORT:-9411}:9411"
    environment:
      COLLECTOR_ZIPKIN_HOST_PORT: :9411
      COLLECTOR_OTLP_ENABLED: true
    networks:
      - alphintra-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:16686/"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging: *default-logging
    restart: unless-stopped

# ===============================
# NETWORKS
# ===============================
networks:
  alphintra-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.25.0.0/16
          gateway: 172.25.0.1

# ===============================
# VOLUMES
# ===============================
volumes:
  postgres_data:
    driver: local
  timescaledb_data:
    driver: local
  redis_master_data:
    driver: local
  redis_replica_data:
    driver: local
  zookeeper_data:
    driver: local
  zookeeper_logs:
    driver: local
  kafka_data:
    driver: local
  mlflow_data:
    driver: local
  mlflow_artifacts:
    driver: local
  minio_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local