# Production Environment Override for Alphintra Trading Platform
# This file extends the base configuration with production-specific settings

version: '3.8'

x-production-logging: &production-logging
  driver: "json-file"
  options:
    max-size: "50m"
    max-file: "5"
    labels: "service,environment"

x-resource-limits: &default-resources
  deploy:
    resources:
      limits:
        memory: 1G
      reservations:
        memory: 512M

x-restart-policy: &restart-policy
  restart_policy:
    condition: on-failure
    delay: 5s
    max_attempts: 3
    window: 120s

services:
  # ===============================
  # PRODUCTION INFRASTRUCTURE
  # ===============================

  # Production PostgreSQL with performance tuning
  postgres:
    <<: *default-resources
    environment:
      POSTGRES_DB: alphintra_prod
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_INITDB_ARGS: "--encoding=UTF-8 --lc-collate=C --lc-ctype=C"
    command: 
      - postgres
      - -c
      - max_connections=200
      - -c
      - shared_buffers=256MB
      - -c
      - effective_cache_size=1GB
      - -c
      - maintenance_work_mem=64MB
      - -c
      - checkpoint_completion_target=0.9
      - -c
      - wal_buffers=16MB
      - -c
      - default_statistics_target=100
      - -c
      - random_page_cost=1.1
      - -c
      - effective_io_concurrency=200
      - -c
      - work_mem=4MB
      - -c
      - min_wal_size=1GB
      - -c
      - max_wal_size=4GB
      - -c
      - max_worker_processes=8
      - -c
      - max_parallel_workers_per_gather=4
      - -c
      - max_parallel_workers=8
      - -c
      - max_parallel_maintenance_workers=4
    volumes:
      - postgres_prod_data:/var/lib/postgresql/data
      - ./backups/postgres:/backups
    logging: *production-logging
    deploy:
      <<: *restart-policy
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'

  # Production TimescaleDB with compression
  timescaledb:
    <<: *default-resources
    environment:
      POSTGRES_DB: ${TIMESCALE_DB}
      POSTGRES_USER: ${TIMESCALE_USER}
      POSTGRES_PASSWORD: ${TIMESCALE_PASSWORD}
    command: 
      - postgres
      - -c
      - shared_preload_libraries=timescaledb
      - -c
      - max_connections=100
      - -c
      - shared_buffers=512MB
      - -c
      - effective_cache_size=2GB
      - -c
      - work_mem=8MB
      - -c
      - timescaledb.max_background_workers=8
    volumes:
      - timescaledb_prod_data:/var/lib/postgresql/data
      - ./backups/timescaledb:/backups
    logging: *production-logging
    deploy:
      <<: *restart-policy
      resources:
        limits:
          memory: 3G
          cpus: '1.5'
        reservations:
          memory: 1.5G
          cpus: '0.75'

  # Production Redis Cluster
  redis-master:
    <<: *default-resources
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD} --maxmemory 1gb --maxmemory-policy allkeys-lru --save 900 1 --save 300 10 --save 60 10000
    volumes:
      - redis_prod_master_data:/data
      - ./backups/redis:/backups
    logging: *production-logging
    deploy:
      <<: *restart-policy
      resources:
        limits:
          memory: 1.5G
          cpus: '0.5'
        reservations:
          memory: 1G
          cpus: '0.25'

  redis-replica:
    <<: *default-resources
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD} --replicaof redis-master 6379 --masterauth ${REDIS_PASSWORD} --replica-read-only yes
    volumes:
      - redis_prod_replica_data:/data
    logging: *production-logging
    deploy:
      <<: *restart-policy

  # Production Kafka with optimized settings
  kafka:
    <<: *default-resources
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:${KAFKA_PORT:-9092}
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:${KAFKA_PORT:-9092}
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
      KAFKA_NUM_PARTITIONS: 6
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_LOG_RETENTION_BYTES: 107374182400
      KAFKA_NUM_NETWORK_THREADS: 8
      KAFKA_NUM_IO_THREADS: 16
      KAFKA_SOCKET_SEND_BUFFER_BYTES: 102400
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: 102400
      KAFKA_SOCKET_REQUEST_MAX_BYTES: 104857600
      KAFKA_COMPRESSION_TYPE: lz4
      KAFKA_BATCH_SIZE: 65536
      KAFKA_LINGER_MS: 10
      KAFKA_JMX_PORT: 9999
      KAFKA_JMX_HOSTNAME: kafka
    volumes:
      - kafka_prod_data:/var/lib/kafka/data
      - ./backups/kafka:/backups
    logging: *production-logging
    deploy:
      <<: *restart-policy
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'

  # ===============================
  # PRODUCTION APPLICATION SERVICES
  # ===============================

  # Production Gateway
  gateway:
    build:
      context: ../../src/backend/gateway
      dockerfile: Dockerfile.prod
    image: alphintra/gateway:${VERSION:-latest}
    <<: *default-resources
    environment:
      SPRING_PROFILES_ACTIVE: prod
      REDIS_HOST: redis-master
      REDIS_PASSWORD: ${REDIS_PASSWORD}
      JWT_SECRET: ${JWT_SECRET}
      CORS_ALLOWED_ORIGINS: ${CORS_ALLOWED_ORIGINS}
      MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE: health,info,metrics,prometheus
      MANAGEMENT_ENDPOINT_HEALTH_SHOW_DETAILS: when_authorized
      LOGGING_LEVEL_COM_alphintra: INFO
      LOGGING_LEVEL_ORG_SPRINGFRAMEWORK_CLOUD_GATEWAY: INFO
      JAVA_OPTS: >-
        -server
        -Xms512m
        -Xmx1g
        -XX:+UseG1GC
        -XX:+UseStringDeduplication
        -XX:+OptimizeStringConcat
        -Djava.security.egd=file:/dev/./urandom
        -Dspring.backgroundpreinitializer.ignore=true
    logging: *production-logging
    deploy:
      <<: *restart-policy
      resources:
        limits:
          memory: 1.5G
          cpus: '1.0'
        reservations:
          memory: 768M
          cpus: '0.5'

  # Production Auth Service
  auth-service:
    build:
      context: ../../src/backend/auth-service
      dockerfile: Dockerfile.prod
    image: alphintra/auth-service:${VERSION:-latest}
    <<: *default-resources
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/alphintra_prod
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis-master:6379/0
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      JWT_SECRET: ${JWT_SECRET}
      SMTP_HOST: ${SMTP_HOST}
      SMTP_PORT: ${SMTP_PORT}
      SMTP_USERNAME: ${SMTP_USERNAME}
      SMTP_PASSWORD: ${SMTP_PASSWORD}
      ENVIRONMENT: production
      LOG_LEVEL: INFO
      SENTRY_DSN: ${SENTRY_DSN}
      NEW_RELIC_LICENSE_KEY: ${NEW_RELIC_LICENSE_KEY}
    logging: *production-logging
    deploy:
      <<: *restart-policy

  # Production Trading API
  trading-api:
    build:
      context: ../../src/backend/trading-api
      dockerfile: Dockerfile.prod
    image: alphintra/trading-api:${VERSION:-latest}
    <<: *default-resources
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/alphintra_prod
      TIMESCALEDB_URL: postgresql://${TIMESCALE_USER}:${TIMESCALE_PASSWORD}@timescaledb:5432/${TIMESCALE_DB}
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis-master:6379/1
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      MLFLOW_TRACKING_URI: http://mlflow:5000
      GOOGLE_CLOUD_PROJECT: ${GCP_PROJECT_ID}
      GOOGLE_APPLICATION_CREDENTIALS: /app/credentials/gcp-service-account.json
      ENVIRONMENT: production
      LOG_LEVEL: INFO
      SENTRY_DSN: ${SENTRY_DSN}
      NEW_RELIC_LICENSE_KEY: ${NEW_RELIC_LICENSE_KEY}
      # Exchange API credentials
      BINANCE_API_KEY: ${BINANCE_API_KEY}
      BINANCE_SECRET_KEY: ${BINANCE_SECRET_KEY}
      COINBASE_API_KEY: ${COINBASE_API_KEY}
      COINBASE_SECRET_KEY: ${COINBASE_SECRET_KEY}
      COINBASE_PASSPHRASE: ${COINBASE_PASSPHRASE}
    volumes:
      - ./secrets/gcp-service-account.json:/app/credentials/gcp-service-account.json:ro
    logging: *production-logging
    deploy:
      <<: *restart-policy
      resources:
        limits:
          memory: 2G
          cpus: '1.5'
        reservations:
          memory: 1G
          cpus: '0.75'

  # Production Strategy Engine
  strategy-engine:
    build:
      context: ../../src/backend/strategy-engine
      dockerfile: Dockerfile.prod
    image: alphintra/strategy-engine:${VERSION:-latest}
    <<: *default-resources
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/alphintra_prod
      TIMESCALEDB_URL: postgresql://${TIMESCALE_USER}:${TIMESCALE_PASSWORD}@timescaledb:5432/${TIMESCALE_DB}
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis-master:6379/2
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      MLFLOW_TRACKING_URI: http://mlflow:5000
      ENVIRONMENT: production
      LOG_LEVEL: INFO
      SENTRY_DSN: ${SENTRY_DSN}
    logging: *production-logging
    deploy:
      <<: *restart-policy
      resources:
        limits:
          memory: 2G
          cpus: '2.0'
        reservations:
          memory: 1G
          cpus: '1.0'

  # Production Broker Connector
  broker-connector:
    build:
      context: ../../src/backend/broker-connector
      dockerfile: Dockerfile.prod
    image: alphintra/broker-connector:${VERSION:-latest}
    <<: *default-resources
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/alphintra_prod
      TIMESCALEDB_URL: postgresql://${TIMESCALE_USER}:${TIMESCALE_PASSWORD}@timescaledb:5432/${TIMESCALE_DB}
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis-master:6379/3
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      ENVIRONMENT: production
      LOG_LEVEL: INFO
      SENTRY_DSN: ${SENTRY_DSN}
      # Exchange API credentials
      BINANCE_API_KEY: ${BINANCE_API_KEY}
      BINANCE_SECRET_KEY: ${BINANCE_SECRET_KEY}
      COINBASE_API_KEY: ${COINBASE_API_KEY}
      COINBASE_SECRET_KEY: ${COINBASE_SECRET_KEY}
      COINBASE_PASSPHRASE: ${COINBASE_PASSPHRASE}
    logging: *production-logging
    deploy:
      <<: *restart-policy

  # ===============================
  # PRODUCTION MONITORING
  # ===============================

  # Production Prometheus with remote storage
  prometheus:
    <<: *default-resources
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--storage.tsdb.retention.size=50GB'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
      - '--web.external-url=https://prometheus.alphintra.com'
    volumes:
      - ./monitoring/prometheus/prometheus.prod.yml:/etc/prometheus/prometheus.yml
      - ./monitoring/prometheus/rules:/etc/prometheus/rules
      - prometheus_prod_data:/prometheus
    logging: *production-logging
    deploy:
      <<: *restart-policy
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'

  # Production Grafana
  grafana:
    <<: *default-resources
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}
      GF_USERS_ALLOW_SIGN_UP: false
      GF_SERVER_ROOT_URL: https://grafana.alphintra.com
      GF_SECURITY_SECRET_KEY: ${GRAFANA_SECRET_KEY}
      GF_DATABASE_TYPE: postgres
      GF_DATABASE_HOST: postgres:5432
      GF_DATABASE_NAME: grafana
      GF_DATABASE_USER: ${POSTGRES_USER}
      GF_DATABASE_PASSWORD: ${POSTGRES_PASSWORD}
      GF_SESSION_PROVIDER: redis
      GF_SESSION_PROVIDER_CONFIG: addr=redis-master:6379,pool_size=100,db=5,password=${REDIS_PASSWORD}
      GF_INSTALL_PLUGINS: grafana-clock-panel,grafana-simple-json-datasource,grafana-worldmap-panel,grafana-piechart-panel
      GF_FEATURE_TOGGLES_ENABLE: ngalert
      GF_SMTP_ENABLED: true
      GF_SMTP_HOST: ${SMTP_HOST}:${SMTP_PORT}
      GF_SMTP_USER: ${SMTP_USERNAME}
      GF_SMTP_PASSWORD: ${SMTP_PASSWORD}
      GF_SMTP_FROM_ADDRESS: grafana@alphintra.com
    volumes:
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards
    logging: *production-logging
    deploy:
      <<: *restart-policy

  # AlertManager for production alerts
  alertmanager:
    image: prom/alertmanager:latest
    container_name: alphintra-alertmanager-prod
    ports:
      - "9093:9093"
    volumes:
      - ./monitoring/alertmanager/alertmanager.prod.yml:/etc/alertmanager/alertmanager.yml
      - alertmanager_prod_data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=https://alertmanager.alphintra.com'
    networks:
      - alphintra-network
    logging: *production-logging
    deploy:
      <<: *restart-policy
    restart: unless-stopped

  # ===============================
  # PRODUCTION SECURITY
  # ===============================

  # Vault for secrets management
  vault:
    image: vault:latest
    container_name: alphintra-vault-prod
    cap_add:
      - IPC_LOCK
    ports:
      - "8200:8200"
    environment:
      VAULT_DEV_ROOT_TOKEN_ID: ${VAULT_ROOT_TOKEN}
      VAULT_DEV_LISTEN_ADDRESS: 0.0.0.0:8200
      VAULT_API_ADDR: http://vault:8200
    volumes:
      - vault_prod_data:/vault/data
      - ./config/vault:/vault/config
    command: vault server -config=/vault/config/vault.hcl
    networks:
      - alphintra-network
    logging: *production-logging
    deploy:
      <<: *restart-policy
    restart: unless-stopped

# ===============================
# PRODUCTION VOLUMES
# ===============================
volumes:
  postgres_prod_data:
    driver: local
  timescaledb_prod_data:
    driver: local
  redis_prod_master_data:
    driver: local
  redis_prod_replica_data:
    driver: local
  kafka_prod_data:
    driver: local
  prometheus_prod_data:
    driver: local
  alertmanager_prod_data:
    driver: local
  vault_prod_data:
    driver: local